{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "   <a target=\"_blank\" href=\"https://www.clarifai.com/\" ><img src=\"https://upload.wikimedia.org/wikipedia/commons/b/bc/Clarifai_Logo_FC_Web.png\" width=256/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "<a href=\"https://colab.research.google.com/github/Clarifai/examples/blob/main/Integrations/Langchain/Chains/Retrieval_QA_chain_with_Clarifai_Vectorstore.ipynb\" target=\"_blank\"><img\n",
    "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Colab\"></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nl0hWwkBF7iA"
   },
   "source": [
    "# Retrieval QA chain with Clarifai Vectorstore using different Chain types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PecScw_zF-y9"
   },
   "source": [
    "A retrieval QA chain is a Question-answering system that uses a retrieval-based approach to find relevant information from a large collection of documents stored inside your clarifai vectorstore. It consists of two main components: a retriever and a language model. Also, this example walks through the scenarios where the retriever generates response based on the chain types.\n",
    "\n",
    "By leveraging langchain's retriever and chains , we can build a simple Q/A with our vectorstore which answers user queries based on the relevant content chunks from retriever.\n",
    "\n",
    "*    The retriever is responsible for searching the document collection and retrieving the most relevant documents based on the input query.\n",
    "\n",
    "*   The language model is then used to generate an answer based on the retrieved documents.\n",
    "\n",
    "* [Chain type](https://python.langchain.com/docs/modules/chains/document/), which basically decides the sequence of operation to be performed with respect to retrieved chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9l7gms8HHAYr"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_R8q5WJWHFcA"
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install clarifai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPs9ZTfaGoEY"
   },
   "source": [
    "Initialize a LLM class from clarifai models.\n",
    "\n",
    "You can use several language models from [Clarifai](https://clarifai.com/explore/models?filterData=%5B%7B%22field%22%3A%22use_cases%22%2C%22value%22%3A%5B%22llm%22%5D%7D%5D&page=1&perPage=24) platform. Sign up and get your [PAT](https://clarifai.com/settings/security) to access it.\n",
    "\n",
    "There's 2 ways to call a model from clarifai platform, either from the model URL or using the model's app id, user id, model id credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m5EkIwQ5GgJj"
   },
   "outputs": [],
   "source": [
    "# use model URL\n",
    "MODEL_URL=\"https://clarifai.com/mistralai/completion/models/mistral-7B-OpenOrca\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQyaoAYuUKfz"
   },
   "source": [
    "For the example we are using ***mistral-7B-OpenOrca*** model.\n",
    "\n",
    "## **mistral-7B-OpenOrca**\n",
    "This [mistral-7B-OpenOrca](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca) release is a finetune of Mistral-7B base model. It claims to achieve 98% of the eval performance of Llama2-70B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmycIp8hT1SW"
   },
   "source": [
    "Initialize your PAT key as env variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0GVifse8TwU9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CLARIFAI_PAT\"]=\"YOUR_CLARIFAI_PAT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set_debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "set_debug(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the global debug flag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNHf4Y0CUBI9"
   },
   "source": [
    "Import Clarifai LLM class from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "cM0zX0COTyTa"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Clarifai\n",
    "llm=Clarifai(model_url=MODEL_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjKhmGqeHQVq"
   },
   "source": [
    "### Pre-Processing of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcLZA0Z3azm7"
   },
   "source": [
    "To ingest your document into the vectorstore, Pre-process it using langchain's TextSplitter and Textloader funtions to convert and load it into Clarifai vectorstore as documents of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cODrnzEvD0m4"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Clarifai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_5c99OgGEKJ"
   },
   "source": [
    "For the context we are ingesting a handbook of *Gulf historic Dubai GP revival - 2023 rule book* and we want to chat with it to understand the rules and regulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEybsmWuGFA5"
   },
   "outputs": [],
   "source": [
    "loader = TextLoader(\"../Data/rule_book.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm34A_bIGLhx"
   },
   "source": [
    "### **Why Clarifai vectorstore ?**\n",
    "Clarifai Vectorstore combines the process of embedding your text inputs and storing in vectoredatabase. The Clarifai vectorstore - langchain [integration](https://python.langchain.com/docs/integrations/vectorstores/clarifai) makes it easier to store inputs as vectors, Which effectively reduce the need for an embedding model and focusses on uploading documents directly into clarifai vectorstore DB and perform effective vector search Q/A with your documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6ZkZTNmGIEN"
   },
   "outputs": [],
   "source": [
    "clarifai_vector_db = Clarifai.from_documents(\n",
    "    user_id=\"user_id\",\n",
    "    app_id= \"app_id\",\n",
    "    documents=docs,\n",
    "    number_of_docs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkLwLsFVLCFf"
   },
   "source": [
    "### Chain_type\n",
    "[Chain_type](https://python.langchain.com/docs/modules/chains/document/) is indeed an important parameter in your retrieval QA apps where it will decide the format and method how the retrieved chunks has been processed before generating outputs.\n",
    "Let's test our Vectorstore with retrievalQA chain with different chain types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ruq8xzw2yfD4"
   },
   "source": [
    "### **Retrieval QA chain with chain_type as Stuff**\n",
    "\n",
    "[Stuff](https://python.langchain.com/docs/modules/chains/document/stuff) is simple method where it retrieves all the relevant document chunks and stuffs it as a whole in the prompt and send it to our LLM to generate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaNKB0YXGOwY"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=clarifai_vector_db.as_retriever())\n",
    "query = \"How automobile classes has been segregated with respect to engines?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__ew1AJ7V04C"
   },
   "source": [
    "In the above scenario we have used a chain type of stuff, which effectively stuffs all the relevant chunks which was retrieved from the vector store into LLM model's context as a prompt. This in return generates a response according to the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7ba7kx3JEV9",
    "outputId": "9520357a-b06d-4a75-c51e-0d04a053f829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The automobile classes have been segregated with respect to engines based on the type of engine used in the Formula One cars from 1970 to 1985. The classes are as follows:\n",
      "\n",
      "1. F1 Grand Prix cars 3L aspirated engine built from 1970 to 1972, equipped with a Ford-Cosworth DFV engine (Class 4.4.1.1).\n",
      "2. F1 Grand Prix cars 3L aspirated engine built from 1973 to 1976, equipped with a Ford-Cosworth DFV engine (Class 4.4.1.2).\n",
      "3. F1 Grand Prix cars 3L aspirated engine built from 1977 to 1980, designed not to exploit the ground effect (Class 4.5.1.1).\n",
      "4. F1 Grand Prix cars 3L aspirated engine built from 1977 to 1980, designed to exploit the ground effect, equipped with a Ford-Cosworth DFV engine (Class 4.5.1.2).\n",
      "5. F1 Grand Prix cars 3L aspirated engine built from 1977 to 1980, designed to exploit the ground effect, equipped with other engines (Class 4.5.1.3).\n",
      "6. F1 Grand Prix cars 3L aspirated engine built from 1981 to 1985, equipped with a Ford-Cosworth DFV engine (Class 4.6.1.1).\n",
      "7. F1 Grand Prix cars 3L aspirated engine built from 1981 to 1985, equipped with other engines (Class 4.6.1.2).\n",
      "\n",
      "These classes are further divided based on the type of engine used and whether the car was designed to exploit or not exploit the ground effect.\n"
     ]
    }
   ],
   "source": [
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VH9hEXsF2ncW"
   },
   "source": [
    "**Use case for Stuff :**\n",
    "\n",
    "Stuff is more clear straightforward use case, when the context is smaller and you need precise answering to your Q/A, go with stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STof3T1TzxhE"
   },
   "source": [
    "### **Retrieval QA chain with chain_type as Map Reduce**\n",
    "\n",
    "[Map Reduce](https://python.langchain.com/docs/modules/chains/document/map_reduce) chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step). It can optionally first compress, or collapse, the mapped documents to make sure that they fit in the combine documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "x2NKp4Wped0w"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=clarifai_vector_db.as_retriever())\n",
    "query = \"How automobile classes has been segregated with respect to engines?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPY_LKP5eidE",
    "outputId": "f478587d-119a-4676-b4d1-615e859e4c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automobile classes have been segregated with respect to engines as follows:\n",
      "\n",
      "1. F1 Grand Prix cars 3L aspirated engine built from 1970 to 1972: \n",
      "   - Equipped with a Ford-Cosworth DFV Engine\n",
      "   - Equipped with other engines\n",
      "\n",
      "2. F1 Grand Prix cars 3L aspirated engine built from 1973 to 1976:\n",
      "   - Equipped with a Ford-Cosworth DFV Engine\n",
      "   - Equipped with other engines\n",
      "\n",
      "3. F1 Grand Prix cars 3L aspirated engine built from 1977 to 1980:\n",
      "   - Cars designed not to exploit the ground effect\n",
      "   - Cars designed to exploit the ground effect, equipped with a Ford-Cosworth DFV engine\n",
      "   - Cars designed to exploit the ground effect, equipped with other engines\n",
      "\n",
      "4. Cars equipped with a Ford-Cosworth DFV engine\n",
      "   - F1 Grand Prix cars 3L aspirated built from 1981 to 1985\n",
      "   - Cars equipped with other engines\n",
      "\n",
      "5. Class Special Invitation: This class is for any Formula One cars or any other cars considered by the Organization to be of Special Historical interest to the Organization or Promoters of any of the races, or of particular benefit to the Organization.\n"
     ]
    }
   ],
   "source": [
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04bHNGTH0d9o"
   },
   "source": [
    "**Use case of Map reduce:**\n",
    "\n",
    "For the same query we got different response while using the Map_reduce chain type, since it is summarizing at each stage before passing it to final stage this chain might be good use case for large context scenarios where we have number of chunks as documents that needs to be answered based on the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IU3-FNWf04Bk"
   },
   "source": [
    "### **Retrieval QA chain with chain_type as Map Re-rank**\n",
    "\n",
    "[Map re-rank](https://python.langchain.com/docs/modules/chains/document/map_rerank) documents chain runs an initial prompt on each document, that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest scoring response is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4UaF7mbdeyVx"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_rerank\", retriever=clarifai_vector_db.as_retriever())\n",
    "query = \"How automobile classes has been segregated with respect to engines?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FragSAiitjql",
    "outputId": "5f933a72-2fae-4b11-ff41-3840046d99aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is also a Special Invitation Class for any Formula One cars or any other cars considered to be of Special Historical interest to the Organization or Promoters of any of the races, or particular benefit to the Organization.\n"
     ]
    }
   ],
   "source": [
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3ledCWn1PI_"
   },
   "source": [
    "**Use case for Map Re-rank:**\n",
    "\n",
    "From the above response we can see the generated answer is based on the top ranked chunk summary from vectorstore, which is then passed to LLM to generate response.\n",
    "It can be effectively used in long context scenarios and Top K was larger also the documents might contain several chunks with same topics.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarifai Resources\n",
    "\n",
    "**Website**: [https://www.clarifai.com](https://www.clarifai.com/)\n",
    "\n",
    "**Demo**: [https://clarifai.com/demo](https://clarifai.com/demo)\n",
    "\n",
    "**Sign up for a free Account**: [https://clarifai.com/signup](https://clarifai.com/signup)\n",
    "\n",
    "**Developer Guide**: [https://docs.clarifai.com](https://docs.clarifai.com/)\n",
    "\n",
    "**Clarifai Community**: [https://clarifai.com/explore](https://clarifai.com/explore)\n",
    "\n",
    "**Python SDK Docs**: [https://docs.clarifai.com/python-sdk/api-reference](https://docs.clarifai.com/python-sdk/api-reference)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
