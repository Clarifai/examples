# This is the sample config file for the llama model.

model:
  id: "gemma-3-4b-it"
  user_id: "user_id"
  app_id: "app_id"
  model_type_id: "multimodal-to-text"

build_info:
  python_version: '3.11'

inference_compute_info:
  cpu_limit: '3'
  cpu_memory: 14Gi
  num_accelerators: 1
  accelerator_type:
  - NVIDIA-A10G
  - NVIDIA-L40S
  - NVIDIA-A100
  - NVIDIA-H100
  accelerator_memory: 22Gi

checkpoints:
  type: huggingface
  repo_id: google/gemma-3-4b-it
  hf_token: "token"
  when: runtime
