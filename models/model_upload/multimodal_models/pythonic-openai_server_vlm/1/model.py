import os
import sys

sys.path.append(os.path.dirname(__file__))
from typing import List

from clarifai.runners.models.model_builder import ModelBuilder
from clarifai.runners.models.model_class import ModelClass
from clarifai.runners.utils.data_types import Image, Stream
from openai import OpenAI
from openai_client_wrapper import OpenAIWrapper
from openai_server_starter import OpenAI_APIServer


class MyRunner(ModelClass):
  """
  A custom runner that integrates with the Clarifai platform and uses Server inference
  to process inputs, including text and images.
  """

  def load_model(self):
    """Load the model here and start the  server."""
    os.path.join(os.path.dirname(__file__))

    server_args = {
        'modalities': ['image'],
        # Increase this to use more image in chat.
        'limit_mm_per_prompt': 'image=10',
        'max_model_len': 2048,
        'gpu_memory_utilization': 0.8,
        'dtype': 'auto',
        'task': 'auto',
        'kv_cache_dtype': 'auto',
        'tensor_parallel_size': 1,
        'quantization': None,
        'chat_template': None,
        'cpu_offload_gb': 0.0,
        'port': 23333,
        'host': 'localhost',
        'checkpoints': "runtime"
    }

    # if checkpoints == "checkpoints" => assign to checkpoints var aka local checkpoints path
    stage = server_args.get("checkpoints")
    if stage in ["build", "runtime"]:
      #checkpoints = os.path.join(os.path.dirname(__file__), "checkpoints")
      config_path = os.path.dirname(os.path.dirname(__file__))
      builder = ModelBuilder(config_path, download_validation_only=True)
      checkpoints = builder.download_checkpoints(stage=stage)
      server_args.update({"checkpoints": checkpoints})

    if server_args.get("additional_list_args") == ['']:
      server_args.pop("additional_list_args")

    modalities = server_args.pop("modalities", ["image", "audio", "video"])
    if not modalities:
      modalities = ["image", "audio", "video"]
    print("Model: Accepted modalities: ", modalities)
    # Start server
    # This line were generated by `upload` module
    self.server = OpenAI_APIServer.from_vllm_backend(**server_args)
    # Create client
    self.client = OpenAIWrapper(
        client=OpenAI(
            api_key="notset",
            base_url=OpenAIWrapper.make_api_url(self.server.host, self.server.port)),
        modalities=modalities)

  @ModelClass.method
  def predict(self,
              prompt: str,
              image: Image = None,
              images: List[Image] = None,
              chat_history: List[dict] = None,
              max_tokens: int = 512,
              temperature: float = 0.7,
              top_p: float = 0.8) -> str:
    """This is the method that will be called when the runner is run. It takes in an input and
    returns an output.
    """
    return self.client.chat(
        prompt=prompt,
        image=image,
        images=images,
        messages=chat_history,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p).choices[0].message.content

  @ModelClass.method
  def generate(self,
               prompt: str,
               image: Image = None,
               images: List[Image] = None,
               chat_history: List[dict] = None,
               max_tokens: int = 512,
               temperature: float = 0.7,
               top_p: float = 0.8) -> Stream[str]:
    """Example yielding a whole batch of streamed stuff back."""
    for chunk in self.client.chat(
        prompt=prompt,
        image=image,
        images=images,
        messages=chat_history,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        stream=True):
      if chunk.choices:
        text = (chunk.choices[0].delta.content
                if (chunk and chunk.choices[0].delta.content) is not None else '')
        yield text

  @ModelClass.method
  def chat(self,
           messages: List[dict],
           max_tokens: int = 512,
           temperature: float = 0.7,
           top_p: float = 0.8) -> Stream[dict]:
    """Chat with the model."""
    for chunk in self.client.chat(
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        stream=True):
      yield chunk.to_dict()
