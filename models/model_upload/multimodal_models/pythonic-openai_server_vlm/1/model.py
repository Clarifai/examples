import os
import sys

sys.path.append(os.path.dirname(__file__))
from typing import List

from clarifai.runners.models.model_builder import ModelBuilder
from clarifai.runners.models.model_class import ModelClass
from clarifai.runners.utils.data_types import Image, Stream
from openai import OpenAI
from openai_client_wrapper import OpenAIWrapper
from openai_server_starter import OpenAI_APIServer

##################


class MyRunner(ModelClass):
  """
  A custom runner that integrates with the Clarifai platform and uses Server inference
  to process inputs, including text and images.
  """

  def load_model(self):
    """Load the model here and start the  server."""
    os.path.join(os.path.dirname(__file__))
    # Use downloaded checkpoints.
    # Or if you intend to download checkpoint at runtime, set hf id instead. For example:
    # checkpoints = "Qwen/Qwen2-7B-Instruct"

    # server args were generated by `upload` module
    server_args = {
        'modalities': ['image'],

        # Increase this to use more image in chat.
        'limit_mm_per_prompt': 'image=10',

        # Leave it as None to use full model length (128k) but need more GPU mem.
        'max_model_len': None,
        'gpu_memory_utilization': 0.91,
        'dtype': 'half',
        'task': 'auto',
        'kv_cache_dtype': 'auto',
        'tensor_parallel_size': 1,
        'chat_template': None,
        'cpu_offload_gb': 0.0,
        'quantization': None,
        'port': 23333,
        'host': 'localhost',
        'checkpoints': "runtime"
    }

    # if checkpoints == "checkpoints" => assign to checkpoints var aka local checkpoints path
    stage = server_args.get("checkpoints")
    if stage in ["build", "runtime"]:
      #checkpoints = os.path.join(os.path.dirname(__file__), "checkpoints")
      config_path = os.path.dirname(os.path.dirname(__file__))
      builder = ModelBuilder(config_path, download_validation_only=True)
      checkpoints = builder.download_checkpoints(stage=stage)
      server_args.update({"checkpoints": checkpoints})

    if server_args.get("additional_list_args") == ['']:
      server_args.pop("additional_list_args")

    modalities = server_args.pop("modalities", ["image", "audio", "video"])
    if not modalities:
      modalities = ["image", "audio", "video"]
    print("Model: Accepted modalities: ", modalities)
    # Start server
    # This line were generated by `upload` module
    self.server = OpenAI_APIServer.from_vllm_backend(**server_args)

    # Create client
    self.client = OpenAIWrapper(
        client=OpenAI(
            api_key="notset",
            base_url=OpenAIWrapper.make_api_url(self.server.host, self.server.port)),
        modalities=modalities)

  @ModelClass.method
  def predict(self,
              prompt: str,
              image: Image = None,
              images: List[Image] = None,
              chat_history: List[dict] = None,
              max_tokens: int = 512,
              temperature: int = 0.7,
              top_p: float = 0.8) -> str:
    """This is the method that will be called when the runner is run. It takes in an input and
    returns an output.
    """
    return self.client.predict(
        prompt=prompt,
        image=image,
        images=images,
        chat_history=chat_history,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p)

  @ModelClass.method
  def generate(self,
               prompt: str,
               image: Image = None,
               images: List[Image] = None,
               chat_history: List[dict] = None,
               max_tokens: int = 512,
               temperature: int = 0.7,
               top_p: float = 0.8) -> Stream[str]:
    """Example yielding a whole batch of streamed stuff back."""
    for each in self.client.generate(
        prompt=prompt,
        image=image,
        images=images,
        chat_history=chat_history,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p):
      yield each

  def test(self):
    """Test the model here."""

    # Test predict
    print(
        self.predict(
            prompt="Hello, how are you?",
            image=None,
            images=None,
            chat_history=None,
            max_tokens=512,
            temperature=0.7,
            top_p=0.8))

    # Test generate
    for each in self.generate(
        prompt="Hello, how are you?",
        image=None,
        images=None,
        chat_history=None,
        max_tokens=512,
        temperature=0.7,
        top_p=0.8):
      print(each)
