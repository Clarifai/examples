# Config file for the VLLM runner

model:
  id: "lmdeploy-Qwen_Qwen2_5-7B-Instruct"
  user_id: "your username"
  app_id: "text-generation"
  model_type_id: "text-to-text"

build_info:
  python_version: "3.12"

inference_compute_info:
  cpu_limit: "4"
  cpu_memory: "16Gi"
  num_accelerators: 1
  accelerator_type: ["NVIDIA-A10G"]
  accelerator_memory: "24Gi"

checkpoints:
  type: "huggingface"
  repo_id: "Qwen/Qwen2.5-7B-Instruct" # Or any llm hf id supported by lmdeploy
  hf_token: ""
